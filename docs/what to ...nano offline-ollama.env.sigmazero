# ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
# ┃ SigmaZero environment configuration        ┃
# ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

# API host/port for SQEP-Zero
export SQEP_HOST=0.0.0.0
export SQEP_PORT=8080

# Narrator connection (Ollama)
export SQEP_NARRATOR_URL="http://127.0.0.1:11434/api/generate"
export SQEP_NARRATOR_MODEL="tinyllama"

# (Optional) Extra prompt tuning
export SQEP_NARRATOR_EXTRA="--num-predict 48 --temperature 0.15 --top-k 20 --top-p 0.9"

# Local binary/model fallback (if ever used)
export SQEP_LLM_BIN="src/ai/llm/llama-simple-chat"
export SQEP_LLM_MODEL="src/ai/llm/tinyllama.stable.gguf"

# Add your LD_LIBRARY_PATH if needed
export LD_LIBRARY_PATH="$PWD/src/ai/llm:${LD_LIBRARY_PATH:-}"

